---
title: "TNM Stan models"
date: "`r Sys.Date()`"
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  cache = TRUE,
  message = FALSE,
  autodep = TRUE,
  warning = FALSE,
  cache.lazy = TRUE,
  cache.comments = TRUE,
  fig.retina = 1
)
```

```{r, message=FALSE}
library(dplyr)
library(ggplot2)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
ctl <- list(adapt_delta = 0.99) # more robust than the default 
library(bayesplot)
# devtools::install_github("seananderson/stanhelpers") # if needed
library(stanhelpers)
library(ggsidekick) # devtools::install_github("seananderson/ggsidekick")
```

# Data

This loads a file containing posterior distributions for six Trophic Niche
Metrics (TNM): dNr, dCr, TA, CD, MNND, and SDNND, from 16 different reef sites.
Data file also contains three reef variables: LFTADen (Lionfish density /100m2),
HASAve (averaged score of habitat complexity), and lionfish removal treatment
(binary, yes/no).

```{r}
d <- read.csv("data/FullCommNoLF.csv")
d <- select(d, -MNND, -SDNND, -CD)
```

# Summarizing the response data 

We can summarize the response data assuming it is log normally distributed:

```{r}
d_logged <- group_by(d, Site) %>%
  mutate_at(vars(dNr:TA), log)

d_means <- d_logged %>%
  summarize_all(mean)

d_sds <- d_logged %>%
  summarise_each(funs(sd), dNr:TA) %>%
  dplyr::select(-Site)
names(d_sds) <- paste0(names(d_sds), "_sd")

d_sum <- data.frame(d_means, d_sds)

d_raw <- d_sum # save

d_sum <- d_sum %>% mutate(HASAve = arm::rescale(HASAve), 
  LFTADen = arm::rescale(log(LFTADen)), # NOTE LOG!!
  RemovTreat = arm::rescale(RemovTreat))

d_sum <- d_sum %>% left_join(readr::read_csv("data/SiteCoords.csv"))
```

Plot to add as suplementary material input trophic niche  distributions

```{r, fig.width=12, fig.height=10}
d_ave <- group_by(d, Site) %>% summarize_all(mean)
d_ave <- select(d_ave, Site:TA) %>% reshape2::melt(id.vars = "Site")
d_post <- select(d, Site:TA) %>% reshape2::melt(id.vars = "Site")

ggplot(d_post, aes(value)) + geom_histogram(bins = 130) +
  facet_grid(Site~variable, scales = "free") +
  geom_vline(data = d_ave, aes(xintercept = value), colour = "red") +
  labs(y = "Count", x = "Response value") +
  theme_sleek()
ggsave("figs/1stposteriors.pdf", width = 8, height = 10)
```

The following plot shows the log transformed response data with our mean of the log transformed response data overlaid with a red line. For the most part these look approximately normally distributed and so our summary makes sense. The one variable that is a little bit off is SDNND. Still, I imagine this is close enough. If we wanted to get fancy, we could summarize these with something more flexible like a Gamma distribution.

```{r, fig.width=12, fig.height=10}
d_sum_melt_mean <- select(d_sum, Site:TA) %>% reshape2::melt(id.vars = "Site")
d_sum_melt_sd <- select(d_sum, Site, dNr_sd:TA_sd) %>% reshape2::melt(id.vars = "Site")
d_melt <- select(d, Site:TA) %>% reshape2::melt(id.vars = "Site")

ggplot(d_melt, aes(log(value))) + geom_histogram(bins = 40) +
  facet_grid(Site~variable, scales = "free") +
  geom_vline(data = d_sum_melt_mean, aes(xintercept = value), colour = "red")
```

# Quick model

Let's fit a quick linear model ignoring the uncertainty:

```{r}
m1 <- lm(dNr ~ (HASAve + LFTADen) * RemovTreat, data = d_sum)
arm::display(m1)
```

# Stan

Same model to check:

```{r}
X <- model.matrix(dNr ~ 0 + (HASAve + LFTADen) * RemovTreat, data = d_sum)
stan_dat <- list(y_meas = d_sum$dNr, tau = d_sum$dNr_sd, N = nrow(d_sum), K = ncol(X), X = X,
  N_new = nrow(d_sum), X_new = X)
```

```{r}
writeLines(readLines("tnm.stan"))
```

```{r, results='hide'}
m_basic <- stan("tnm.stan", data = stan_dat, control = ctl)
```

```{r}
m_basic
```

That looks the same to me. 

# Measurement error model

Now let's use the following model that allows for measurement error on y.

We can write the model as follows:

$$
y_i \sim \mathrm{Normal} \left(y^\mathrm{true}_i, \tau_i^2\right),\\
y_i^\mathrm{true} = \beta_0 + \beta_1 H_i + \beta_2 L_i + \beta_3 R_i + + \beta_4 R_i L_i + \epsilon_{i},\\
\epsilon_i \sim \mathrm{Normal}(0, \sigma^2),
$$

where $y_i$ and $\tau_i$ represent the mean and standard deviation of the log-transformed posterior of one of our response variables for site $i$, $y^\mathrm{true}_i$ represents the true unobserved value of that response variable, the $\beta$'s represent estimated coefficients, and $H_i$, $L_i$, and $R_i$ represent the habitat index variable, the lionfish density, and a binary variable representing whether lionfish were removed, respectively. The variable $\epsilon_i$ represents normally distributed residual error with mean 0 and standard deviation $\sigma$.

"True" may not be the correct term to use here. It's not that it is necessarily "true", but it is the response variable without the measurement uncertainty around it.

```{r}
writeLines(readLines("tnm-meas.stan"))
```

```{r, results='hide'}
m_meas <- stan("tnm-meas.stan", data = stan_dat, 
  pars = c("y", "y_raw"), include = FALSE, control = ctl)
```

```{r}
m_meas
```

We can inspect and plot the output:

```{r stan-check-plots}
posterior <- extract(m_meas, inc_warmup = FALSE, permuted = FALSE, pars = c("y_pred", "y_pred_new"), 
  include = FALSE)
mcmc_trace(posterior)
names(as.data.frame(X))

mcmc_areas(as.matrix(m_meas), regex_pars = "beta")
mcmc_areas(as.matrix(m_basic), regex_pars = "beta")
```

So the posteriors are definitely wider in the case when we allow for the measurement error. 

# Other responses

Now that the above models are working, let's apply them to the various responses. 

The following function will format the data and fit the model for a given response. It also has the option of centering or not centering the removal variable. 

```{r}
newdata_den <- expand.grid(HASAve = 0, 
  LFTADen = seq(min(d_sum$LFTADen), 
    max(d_sum$LFTADen), length.out = 150),
  RemovTreat = c(0.5, -0.5),
  y = 0) # ignore y
newdata_hab <- expand.grid(
  HASAve = seq(min(d_sum$HASAve), max(d_sum$HASAve), length.out = 150), 
  LFTADen = 0,
  RemovTreat = c(0.5, -0.5),
  y = 0) # ignore y
newdata <- bind_rows(newdata_den, newdata_hab)
X_new <- model.matrix(y ~ 0 + (HASAve + LFTADen) * RemovTreat, data = newdata)

fit_tnm <- function(response) {
  f <- paste(response, "~ 0 + (HASAve + LFTADen) * RemovTreat")
  X <- model.matrix(as.formula(f), data = d_sum)
  stan_dat <- list(y_meas = d_sum[, response], tau = d_sum$dNr_sd, N = nrow(d_sum), 
    K = ncol(X), X = X, N_new = nrow(X_new), X_new = X_new)
  m <- stan("tnm-meas.stan", data = stan_dat,
    pars = c("y", "y_raw"), include = FALSE, control = list(adapt_delta = 0.99), 
    iter = 3000, chains = 4)
  m
}
```

Now let's apply the function to each of the responses. 

```{r, results='hide'}
responses <- names(d)[2:4]
out_cent <- lapply(responses, fit_tnm)
names(out_cent) <- responses
```

Here we will extract the posteriors from the models and reformat the output for plotting. Also, we will calculate the effect of lionfish density for the case without removals (-0.5) and for the case with removals (0.5). 

```{r}
stopifnot(identical(mean(d$RemovTreat), 0.5)) # Just in case! 
p <- plyr::ldply(out_cent, stanhelpers::extract_df, output = "wide_df")
pred <- select(p, .id, starts_with("y_pred"), -contains("new")) %>% rename(response = `.id`)
est <- select(p, .id, starts_with("beta")) %>% rename(response = `.id`)
names(est)[2:6] <- names(as.data.frame(X))
est <- mutate(est, LFTADen_w_removal = LFTADen + 0.5 * `LFTADen:RemovTreat`,
  LFTADen_no_removal = LFTADen - 0.5 * `LFTADen:RemovTreat`,
  HASAve_w_removal = HASAve + 0.5 * `HASAve:RemovTreat`,
  HASAve_no_removal = HASAve - 0.5 * `HASAve:RemovTreat`)
est <- reshape2::melt(est) # make a long format for ggplot
```

```{r}
#change var and response names
levels(est$variable) <- list("Habitat with no removal"="HASAve_no_removal","Habitat with removal"="HASAve_w_removal", "Lionfish with no removal"="LFTADen_no_removal", "Lionfish with removal"="LFTADen_w_removal","Lionfish:Removal"="LFTADen:RemovTreat","Habitat:Removal"="HASAve:RemovTreat","Removal"="RemovTreat","Lionfish"="LFTADen","Habitat"="HASAve")
est$response <- as.factor(est$response)
levels(est$response) <- list("Nitrogen range"="dNr","Carbon range"="dCr","Total area"="TA")
```

```{r coefficient-plot, fig.width=6, fig.height=6.5}
cis <- est %>% 
  group_by(response, variable) %>% 
  summarise(l = quantile(value, 0.05),
    m = quantile(value, 0.5),
    u = quantile(value, 0.95)) %>% 
  ungroup()

#arrange response variables for a more "logical" order in plots
est$response = with(est, factor(response, levels = rev(levels(response))))
cis$response = with(cis, factor(response, levels = rev(levels(response))))

labs <- c(0.25, 0.5, 0.75, 1, 1.5, 2, 3, 5)
ggplot(est, aes(variable, value, fill = response, colour = response)) + 
  geom_hline(yintercept = 0, lty = 2) + xlab("") +
  scale_y_continuous(breaks = log(labs), labels = labs, limits = range(log(labs))) +
  geom_violin(position = position_dodge(width = 0.8), alpha = 0.5) +
  coord_flip() +
  viridis::scale_fill_viridis(discrete = TRUE) +
  viridis::scale_color_viridis(discrete = TRUE) +
  theme_sleek() +
  ylab("Coefficient estimate") +
  geom_point(data = cis, aes(x = variable, y = m), col = "grey20", cex = 0.9,
    position = position_dodge(width = 0.8)) +
  geom_linerange(data = cis, aes(x = variable, ymin = l, ymax = u, group = response),
    lwd = 0.4, col = "grey20", position = position_dodge(width = 0.8), inherit.aes = FALSE) +
  guides(colour = guide_legend(reverse=TRUE), fill = guide_legend(reverse=TRUE))
ggsave("figs/tnm-estimates.pdf", width = 6, height = 6.5)
```

In the above plot, I labeled the x axis with the exponentiated versions of the coefficients. These are the multiplicative effects. So, for example, a value of 0.6 means that the response will be 60% of what it was if the predictor increases by 2 standard deviations (or in the case of the treatment, the lionfish are removed).

We can calculate the probability a given coefficient is above or less than 0 (i.e. the multiplicative effect is above or below 1) (because this is a Bayesian model). You can use these values when you report results. We can also calculate lots of other things depending on what would be meaningful (e.g. credible intervals on the effects).

```{r}
sum_table <- est %>% group_by(variable, response) %>% 
  summarize(
    prob_less_0 = round(sum(value < 0)/n(), 2),
    prob_above_0 = round(sum(value > 0)/n(), 2))
knitr::kable(sum_table)
```

Here's what I see:  *(numbers might have changed slightly)*  *TODO NUMBERS DIFFERENT, NOW LOGGED*

- None of these effects are overly strong 
- There's a reasonably high probability that most of the responses are lower in the case of lionfish removals (for a site with average lionfish density) (see the `RemovTreat` effect). For example, there is about a 95% probability this is true for `TA` and about a 88% probability this is true for `CD` and `dCr`.
- There is a fairly high probability (ranging from 0.6 to 0.93) that some of these responses (TA, dCr, dNr) are negatively related with lionfish density (`LFTADen`). This is for an average site, or in other words across all sites including those with and without removals. 
- The effect of lionfish density looks a bit stronger (negative) in the case of no removals `LFTADen_no_removal`, BUT the interaction between lionfish density and treatment is very weak (or at least very uncertain) (see `LFTADen:RemovTreat`).
- There is weak evidence for an effect of the habitat variable on the responses with the exception of positive relationship between the habitat variables and `dNr` and `MMND`, with ~0.98 or 0.99 probability for the latter. (`HASAve`)

# Residuals

```{r residuals, warning=FALSE}
p <- reshape2::melt(pred, variable.name = "site_i", value.name = "predicted")
p <- p %>% group_by(response, site_i) %>%
  summarise(predicted = mean(predicted))

obs <- select(d_sum, Site:TA, lat, lon) %>%
  reshape2::melt(id.vars = c("Site", "lat", "lon"), variable.name = "response", 
    value.name = "observed")

stopifnot(identical(nrow(d_sum), length(unique(p$site_i)))) # just to check
lookup <- data_frame(Site = d_sum$Site, site_i = paste0("y_pred_", seq_len(nrow(d_sum))))
res <- inner_join(p, lookup) %>% inner_join(obs) %>%
  mutate(residual = observed - predicted)

ggplot(res, aes(predicted, observed)) + geom_point() +
  facet_wrap(~response, scales = "free") +
  geom_abline(intercept = 0, slope = 1, lty = 2)

ggplot(res, aes(predicted, residual)) + geom_point() +
  facet_wrap(~response, scales = "free_x") +
  geom_abline(intercept = 0, slope = 0, lty = 2)
ggsave("figs/fitted-vs-obs-tnm-residuals.pdf", width = 9, height = 6.5)

ggplot(res, aes(lon, lat, colour = residual)) + geom_point(size = 4) +
  facet_wrap(~response) +
  scale_color_gradient2()
ggsave("figs/spatial-tnm-residuals.pdf", width = 10, height = 6.5)
```

Looks pretty good.

# Check mixing

```{r}
color_scheme_set("mix-blue-red")
lapply(out_cent, function(x) mcmc_trace(extract(x, inc_warmup = FALSE, permuted = FALSE, 
  pars = c("y_pred", "y_pred_new"), include = FALSE)))

lapply(out_cent, function(x) {
  broom::tidyMCMC(x, rhat = TRUE, ess = TRUE) %>% 
    filter(!grepl("y", term)) %>%
    select(term, rhat, ess) %>%
    mutate(rhat = round(rhat, 2), ess = round(ess, 2))
})
```

Looks great.

# Credible intervals 

```{r}
intervals <- group_by(est, response, variable) %>%
  summarize(
    q0.025 = quantile(value, probs = 0.025),
    q0.25 = quantile(value, probs = 0.25),
    q0.5 = quantile(value, probs = 0.5),
    q0.75 = quantile(value, probs = 0.75),
    q0.975 = quantile(value, probs = 0.975))

intervals_rounded <- intervals %>% 
  mutate_at(vars(q0.025:q0.975), round, digits = 2)

knitr::kable(intervals_rounded)
```

# brms package checks

Similar, but subtly different models as checks (ignore).

```{r}
# library(brms)
# m2 <- brm(dNr ~ 1 + HASAve + LFTADen * RemovTreat, data = d_sum)

# m3 <- brm(dNr | se(dNr_sd) ~ 1 + HASAve + LFTADen * RemovTreat, data = d_sum,
#   prior = c(prior(student_t(5, 0, 2), class = "b"), prior(student_t(5, 0, 5), class = "Intercept")))

# m4 <- brm(dNr | se(dNr_sd) ~ 1 + HASAve + LFTADen * RemovTreat + (1|Site), data = d_sum,
#   control = list(adapt_delta = 0.99),
#   prior = c(prior(student_t(5, 0, 2), class = "b"),
#     prior(student_t(5, 0, 5), class = "Intercept"),
#     prior(student_t(5, 0, 2), class = "sd")))
```

# Predictions

```{r}
pp <- lapply(out_cent, function(x) extract(x)$y_pred_new)
pp <- plyr::ldply(pp, function(x) {
  data.frame(X_new, 
    l = apply(x, 2, quantile, probs = 0.1),
    u = apply(x, 2, quantile, probs = 0.9),
    med = apply(x, 2, median))
})
pp$LFTADen_raw <- exp(pp$LFTADen * 2 * sd(log(d_raw$LFTADen)) + mean(log(d_raw$LFTADen)))
pp$HASAve_raw <- pp$HASAve * 2 * sd(d_raw$HASAve) + mean(d_raw$HASAve)
pp <- mutate(pp, `Lionfish removed` = ifelse(RemovTreat == 0.5, "Yes", "No"))

pp$.id <- as.factor(pp$.id)
levels(pp$.id) <- list("Carbon range"="dCr","Nitrogen range"="dNr","Total area"="TA")

d_sum_long <- d_sum %>% select(-ends_with("sd"), -LFRnAve) %>% 
  tidyr::gather(.id, value, -lat, -lon, -Site, -LFTADen, -HASAve, -RemovTreat)
d_sum_long$LFTADen_raw <- exp(d_sum_long$LFTADen * 2 * sd(log(d_raw$LFTADen)) + mean(log(d_raw$LFTADen)))
d_sum_long$HASAve_raw <- d_sum_long$HASAve * 2 * sd(d_raw$HASAve) + mean(d_raw$HASAve)
d_sum_long <- mutate(d_sum_long, `Lionfish removed` = ifelse(RemovTreat == 0.5, "Yes", "No"))

b_s <- plyr::ldply(out_cent, function(x) {
  data.frame(b1 = median(extract(x)$beta[,which(colnames(X) == "HASAve")]),
    b2 = median(extract(x)$beta[,which(colnames(X) == "LFTADen")]))
})

d_sum_long <- left_join(d_sum_long, b_s)
d_sum_long <- mutate(d_sum_long, value_at_mean_HASAve = value - b1 * HASAve)
d_sum_long <- mutate(d_sum_long, value_at_mean_LFTADen = value - b2 * LFTADen)

d_sum_long$.id <- as.factor(d_sum_long$.id)
levels(d_sum_long$.id) <- list("Carbon range"="dCr","Nitrogen range"="dNr","Total area"="TA")

cols <- c(RColorBrewer::brewer.pal(4, "Greys")[[3]], RColorBrewer::brewer.pal(4, "Blues")[[3]])
g <- filter(pp, HASAve == 0) %>% 
  ggplot(aes(LFTADen_raw, exp(med), colour = `Lionfish removed`)) + 
  facet_wrap(~.id, scales = "free_y") +
  geom_line(lwd = 1) +
  geom_ribbon(aes(ymax = exp(u), ymin = exp(l), fill = `Lionfish removed`), 
    alpha = 0.15, lwd = 0) +
  labs(y = "Response value", x = expression(Lionfish~ density~ (per~ 100~ m^2))) +
  scale_color_manual(values = cols) + scale_fill_manual(values = cols) +
  theme_sleek() +
  geom_point(data = d_sum_long, aes(x = LFTADen_raw,
    y = exp(value_at_mean_HASAve), colour = `Lionfish removed`), inherit.aes = FALSE)
g
ggsave("figs/predictions-den.pdf", width = 12, height = 4)

g <- filter(pp, LFTADen == 0) %>% 
  ggplot(aes(HASAve_raw, exp(med), colour = `Lionfish removed`)) + 
  facet_wrap(~.id, scales = "free_y") +
  geom_line(lwd = 1) +
  geom_ribbon(aes(ymax = exp(u), ymin = exp(l), fill = `Lionfish removed`), 
    alpha = 0.15, lwd = 0) +
  labs(y = "Response value", x = "Habitat complexity score") +
  scale_color_manual(values = cols) + scale_fill_manual(values = cols) +
  theme_sleek() +
  geom_point(data = d_sum_long, aes(x = HASAve_raw,
    y = exp(value_at_mean_LFTADen), colour = `Lionfish removed`), inherit.aes = FALSE)
g
ggsave("figs/predictions-hab.pdf", width = 12, height = 4)
```
